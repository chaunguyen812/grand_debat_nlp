{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grand débat : organisation des services publics - Topics Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON VERSION: 3.6.8\n",
      "SPARK VERSION: 2.4.4\n",
      "SPARK-NLP VERSION: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "spark =  sparknlp.start()\n",
    "\n",
    "## PYSPARK\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline as sparkMlPipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "\n",
    "\n",
    "## SPARK-NLP\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.common import *\n",
    "\n",
    "\n",
    "## PYTHON\n",
    "#Basis\n",
    "import sys\n",
    "import numpy as np\n",
    "import datetime\n",
    "# Text treatment\n",
    "import re\n",
    "import unidecode\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "# Dataviz\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium import plugins as foliumplugins\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(\"PYTHON VERSION: {}\".format(sys.version.split()[0]))\n",
    "print(\"SPARK VERSION: {}\".format(sc.version))\n",
    "print(\"SPARK-NLP VERSION: {}\".format(sparknlp.version()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to extract raw data 2020-01-30 14:03:55.235209\n",
      "Number of rows: 111953\n",
      "Prepare raw data 2020-01-30 14:04:07.511296\n",
      "Numbers of answers before filter: 34981\n",
      "Numbers of answers at the end: 34433\n"
     ]
    }
   ],
   "source": [
    "print(\"Start to extract raw data\", datetime.datetime.now())\n",
    "\n",
    "# Data location\n",
    "bucket ='chau-testbucket'\n",
    "data_key ='ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "# Import data\n",
    "df = spark.read.option(\"header\", \"true\")\\\n",
    "               .option(\"multiLine\", True)\\\n",
    "               .option(\"sep\",\",\")\\\n",
    "               .option(\"escape\",\"\\\"\")\\\n",
    "               .option(\"encoding\",\"utf-8\")\\\n",
    "               .csv(data_location)\n",
    "\n",
    "# If 111953 => ok\n",
    "print(\"Number of rows: {}\".format(df.count()))\n",
    "\n",
    "df = df.withColumn(\"n_row\", F.monotonically_increasing_id())\n",
    "\n",
    "for name in df.columns:\n",
    "    df = df.withColumnRenamed(name, name.split(\"-\")[0].strip() if \"-\" in name else name)\n",
    "\n",
    "# Correct author type    \n",
    "# If author type is null => \"Inconnu\"    \n",
    "df = df.na.fill({'authorType': \"Inconnu\"})\n",
    "\n",
    "# Correct zip-code\n",
    "df = df.withColumn(\"authorZipCode\", F.when(F.length(df.authorZipCode) != 5 ,None).otherwise(df.authorZipCode))\n",
    "\n",
    "# Extract Departement from zip-code\n",
    "df = df.withColumn(\"department\", F.when(F.isnull(df.authorZipCode),None).otherwise(F.substring(df.authorZipCode,1,2)))\n",
    "\n",
    "\n",
    "# Special case: DOM TOM\n",
    "# DOM zip-code start with \"97\", and DOM with \"98\"\n",
    "# In this case the department code correspond to the first three figures of the zip-code\n",
    "df = df.withColumn(\"department\", F.when((df.department==97)|(df.department==98),F.substring(df.authorZipCode,1,3))\\\n",
    "                                           .otherwise(df.department))\n",
    "\n",
    "# Special Case: Corse\n",
    "# Zip-code from Corse start with \"20\"\n",
    "# Then if 20000 <= zip-code < 20200 => department = 2A (South Corse)\n",
    "# If 20200 <= zip-code < 21000 => department = 2B (North Corse)\n",
    "df = df.withColumn(\"department\", F.when((df.department==20)&(df.authorZipCode<20200),\"2A\")\\\n",
    "                                    .otherwise(F.when((df.department==20)&(df.authorZipCode>=20200),\"2B\")\\\n",
    "                                               .otherwise(df.department)))\n",
    "\n",
    "# Remove invalid department :if invalid => null value\n",
    "valid_dep = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\",\n",
    "             \"17\", \"18\", \"19\", \"2A\", \"2B\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\",\n",
    "             \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\",\n",
    "             \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\", \"59\", \"60\", \"61\", \"62\", \"63\",\n",
    "             \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"73\", \"74\", \"75\", \"76\", \"77\", \"78\", \"79\",\n",
    "             \"80\", \"81\", \"82\", \"83\", \"84\", \"85\", \"86\", \"87\", \"88\", \"89\", \"90\", \"91\", \"92\", \"93\", \"94\", \"95\",\n",
    "             \"971\", \"972\", \"973\", \"974\", \"975\", \"976\", \"977\", \"978\", \"984\", \"986\", \"987\", \"988\", \"989\"]\n",
    "\n",
    "df = df.withColumn(\"department\", F.when(df.department.isin(valid_dep),df.department).otherwise(None))\n",
    "\n",
    "\n",
    "# If department invalid => zip-code invalid\n",
    "# So if department == null , => zip-code = null\n",
    "df =  df.withColumn(\"authorZipCode\",F.when(F.isnull(df.department),None).otherwise(df.authorZipCode))\n",
    "\n",
    "# Columns to keep ?\n",
    "\n",
    "# Targeted questions :\n",
    "# QUXVlc3Rpb246MTY5 - Que pensez-vous de l'organisation de l'Etat et des administrations en France ? De quelle manière cette organisation devrait-elle évoluer ?\n",
    "# QUXVlc3Rpb246MjA0 - Estimez-vous avoir accès aux services publics dont vous avez besoin ? => \"Oui\" ou \"Non\"\n",
    "# QUXVlc3Rpb246MTcy - Si non, quels types de services publics vous manquent dans votre territoire et qu'il est nécessaire de renforcer ?\n",
    "# QUXVlc3Rpb246MTc4 - Quand vous pensez à l'évolution des services publics au cours des dernières années, quels sont ceux qui ont évolué de manière positive ?\n",
    "# QUXVlc3Rpb246MTc5 - Quels sont les services publics qui doivent le plus évoluer selon vous ?\n",
    "# QUXVlc3Rpb246MTk1 - Si vous avez été amené à demander un remboursement de soins de santé, pouvez-vous indiquer les éléments de satisfaction et/ou les difficultés rencontrés en précisant, pour chaque point, l'administration concernée :\n",
    "# QUXVlc3Rpb246MTk2 - Si vous avez été amené à faire une demande d'aide pour une situation de handicap, pouvez-vous indiquer les éléments de satisfaction et/ou les difficultés rencontrés en précisant, pour chaque point, l'administration concernée :\n",
    "# QUXVlc3Rpb246MTg5 - Y a-t-il d'autres points sur l'organisation de l'Etat et des services publics sur lesquels vous souhaiteriez vous exprimer\n",
    "\n",
    "select_var = {  \n",
    "                \"n_row\": \"id\",\n",
    "                \"department\": \"department\",\n",
    "                \"authorZipCode\":\"zip_code\",\n",
    "                \"authorType\": \"author_type\",\n",
    "                \"QUXVlc3Rpb246MTY5\": \"q_org\",\n",
    "                \"QUXVlc3Rpb246MjA0\": \"q_access\",\n",
    "                \"QUXVlc3Rpb246MTcy\": \"q_need_more\",\n",
    "                \"QUXVlc3Rpb246MTc4\": \"q_positive_change\",\n",
    "                \"QUXVlc3Rpb246MTc5\": \"q_need_change\",\n",
    "                \"QUXVlc3Rpb246MTk1\": \"q_healthcare_refund\",\n",
    "                \"QUXVlc3Rpb246MTk2\": \"q_disability\",\n",
    "                \"QUXVlc3Rpb246MTg5\": \"q_open\" \n",
    "            }\n",
    "\n",
    "# Select the targeted columns, and rename them\n",
    "df = df.select(*select_var.keys())\n",
    "for k,v in select_var.items():\n",
    "    df = df.withColumnRenamed(k, v)\n",
    "    \n",
    "# df.printSchema()\n",
    "\n",
    "# Columns corresponding to a question\n",
    "questions_col = [name for name in df.columns if name[0]==\"q\"]\n",
    "# Columns not corresponding to a question\n",
    "info_col = [name for name in df.columns if name[0]!=\"q\"]\n",
    "\n",
    "# Descriptives stats\n",
    "# df.groupby(\"zip_code\").agg(F.count(\"id\").alias(\"n\")).orderBy(\"n\", ascending=False).show(truncate=False)\n",
    "# df.groupby(\"author_type\").agg(F.count(\"id\").alias(\"n\")).orderBy(\"n\", ascending=False).show(truncate=False)\n",
    "\n",
    "\n",
    "# How many null values in each column ?\n",
    "# df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in questions_col]).show()\n",
    "# df.select([F.isnull(c).alias(c) for c in [\"q_access\",\"q_need_more\"]]).crosstab(\"q_access\", \"q_need_more\").show()\n",
    "# df.filter((df.q_access==\"Oui\")&(~F.isnull(\"q_need_more\"))).select(\"id\", \"q_need_more\").show(5, truncate=False)\n",
    "\n",
    "\n",
    "print(\"Prepare raw data\",datetime.datetime.now())\n",
    "#target = [\"q_need_more\", \"q_access\"]\n",
    "target = [\"q_need_more\"]\n",
    "df_need = df.select(*(info_col+target))\n",
    "df_need = df_need.dropna(subset=target)\n",
    "\n",
    "# Remove the answers of people who previously said they don't lack access to public services\n",
    "#df_need = df_need.filter(df_need.q_access == \"Non\")\n",
    "\n",
    "# Remove too long answers\n",
    "df_need = df_need.withColumn(\"text_len\", F.length(df_need.q_need_more))\n",
    "print(\"Numbers of answers before filter: {}\".format(df_need.count()))\n",
    "df_need = df_need.filter(df_need.text_len < 1000)\n",
    "\n",
    "print(\"Numbers of answers at the end: {}\".format(df_need.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import StopWords 2020-01-30 14:04:17.204912\n",
      "Clean the text 2020-01-30 14:04:18.104186\n",
      "Extract tokens from text 2020-01-30 14:04:18.141260\n",
      "Tokens are collected 2020-01-30 14:04:18.821422\n",
      "Start Lemmatization 2020-01-30 14:04:18.821590\n",
      "Tokens are lemmatized and clean 2020-01-30 14:04:29.879533\n",
      "+--------------------+--------------------+\n",
      "|         q_need_more|               lemma|\n",
      "+--------------------+--------------------+\n",
      "|En priorité l'hop...|[priorite, hopita...|\n",
      "|Renforcer les res...|[renforcer, resso...|\n",
      "|Administration fi...|[administration, ...|\n",
      "|Créer au niveau e...|[creer, niveau, e...|\n",
      "|Reponse difficile...|[reponse, diffici...|\n",
      "|TOUS. les hopitau...|[hopital, priorit...|\n",
      "|          transports|         [transport]|\n",
      "|Dans la mesure où...|[mesure, etude, g...|\n",
      "|Les déserts médic...|[desert, medecin,...|\n",
      "|           trasports|         [transport]|\n",
      "|Une police et une...|[police, justice,...|\n",
      "| transport en commun| [transport, commun]|\n",
      "|carte grise, au m...|[carte, grise, as...|\n",
      "|L'accès aux papie...|[papier, identite...|\n",
      "|Vu les heures et ...|[heure, jour, ouv...|\n",
      "|Pour faire une ca...|[carte, identite,...|\n",
      "|             Hopital|           [hopital]|\n",
      "|            Les CAF.|               [caf]|\n",
      "|l'hopital, les im...|[hopital, impots,...|\n",
      "|Des établissement...|[etablissement, c...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stopwords\n",
    "\n",
    "print(\"Import StopWords\",datetime.datetime.now())\n",
    "\n",
    "# Stopwords location\n",
    "stpw_key ='stopwords-fr.txt'\n",
    "stpw_location = 's3://{}/{}'.format(bucket, stpw_key)\n",
    "\n",
    "stopwords_fr = sqlContext.read.csv(stpw_location, header=False,  inferSchema=True).collect()\n",
    "stopwords_fr = [ w._c0 for w in stopwords_fr]\n",
    "\n",
    "# Alternatives\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# stopwords_fr = nltk.corpus.stopwords.words('french')\n",
    "\n",
    "# from pyspark.ml.feature import StopWordsRemover\n",
    "# len(StopWordsRemover.loadDefaultStopWords('french'))\n",
    "\n",
    "# Add some custom stop words\n",
    "custom = [\n",
    "          \"jamais\", \n",
    "          \"partout\",\n",
    "          \"idem\",\n",
    "          \"france\", \"français\",\n",
    "          \"pouvoir\", \"peux\", \"peut\", \"pouvons\", \"pourrions\", \"peuvent\", \"pu\", \"pourrait\", \"pourraient\",\n",
    "          \"dire\", \"dit\", \"dites\", \"dits\", \"dite\", \"dis\", \"disons\",\n",
    "          \"devoir\", \"dois\", \"doit\", \"du\", \"dus\", \"dues\", \"due\", \"devons\", \"doivent\", \"devrait\", \"devraient\", \n",
    "          \"voir\", \"vu\", \"voit\", \"voient\", \"vois\", \"vus\", \"vue\", \"vues\",\n",
    "          \"petit\", \"petits\", \"petite\", \"petits\", \"petites\",\n",
    "          \"grand\", \"grande\", \"grands\", \"grandes\",\n",
    "          \"presque\",\n",
    "          \"oui\", \"non\",\n",
    "          \"faire\", \"fais\", \"fait\", \"faisons\", \"faîtes\", \"fait\", \"faits\", \"faite\", \"faites\", \"font\",\n",
    "          \"mettre\", \"mis\", \"met\", \"mets\", \"mise\", \"mises\",\n",
    "          \"prendre\", \"pris\", \"prise\", \"prises\", \"prend\", \"prends\", \"prenons\", \"prennent\", \n",
    "          \"fin\", \"a priori\", \"priori\",\n",
    "          \"face\",\n",
    "          \"savoir\", \"sait\", \"sais\", \"savons\", \"sache\", \"sachent\",\n",
    "          \"permettre\", \"permis\", \"permise\", \"permet\", \"permettent\", \"permises\", \"permetterait\", \"permette\",\n",
    "          \"vouloir\", \"veut\", \"voulait\", \"veux\", \"voulu\", \"voulaient\", \"veulent\", \"voulons\",\n",
    "          \"aller\", \"va\", \"allé\", \"allés\", \"allée\", \"allées\", \"vont\", \n",
    "          \"service\", \"services\",\n",
    "          \"public\", \"publique\", \"publiques\", \"publics\",\n",
    "          \"vis\",\n",
    "          \"end\", \"ends\",\n",
    "          \"accès\", \"besoin\", \"besoins\", \"problèmes\", \"problème\",\n",
    "          \"exemple\", \"exemples\", \"ex\", \"cas\", \"situations\", \"situation\",\n",
    "    \n",
    "    \n",
    "#           \"manque\", \"manquent\", \"manquer\", \"manqué\", \"manquant\", \"manquante\", \"manquantes\", \"manquants\",\n",
    "#           \"suppressions\", \"suppression\", \"supprimer\", \"supprimés\", \"supprimées\", \"supprimée\", \"supprimé\",\n",
    "#           \"disparition\", \"disparitions\", \"disparus\", \"disparu\", \"disparue\", \"disparues\", \"disparaitre\",\n",
    "#           \"difficiles\", \"difficultés\",\n",
    "#           \"problèmes\", \"problème\",\n",
    "#           \"suffisament\",\n",
    "\n",
    "#           \"personnel\", \"personnels\", \"effectif\", \"effectifs\",    \n",
    "#           \"administration\", \"administratif\", \"administrative\", \"administratifs\", \"administratives\",\n",
    "#           \"manque\", \"manquent\", \"manquer\",\n",
    "#           \"renforcer\", \"renforce\", \"renforcé\", \"renforcés\", \"renforcée\", \"renforcées\",\n",
    "#           \"réponse\", \"réponds\", \"répondre\", \"réponses\", \n",
    "#           \"concerne\",\n",
    "#           \"moyen\", \"moyens\", \n",
    "          \n",
    "            ]\n",
    "\n",
    "stopwords_fr += custom\n",
    "\n",
    "# Clean/Correct the text\n",
    "\n",
    "print(\"Clean the text\",datetime.datetime.now())\n",
    "\n",
    "def treat_text(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r'[^\\w\\s]',r' ', string) #^\\x00-\\x7f\n",
    "    string = re.sub(\"\\d+\", \"\", string)\n",
    "    string = string.strip()\n",
    "#     string = string.replace(\"la poste\", \"laposte\")\n",
    "#     string = string.replace(\"rendez vous\", \"rendezvous\")\n",
    "#     string = string.replace(\"rdvs\", \"rendezvous\")\n",
    "#     string = string.replace(\"rdv\", \"rendezvous\")\n",
    "    return string\n",
    "\n",
    "treat_text_udf = F.udf(treat_text, StringType())\n",
    "df_need = df_need.withColumn(\"text\", treat_text_udf(df_need[\"q_need_more\"]))\n",
    "\n",
    "\n",
    "# NLP Pipeline : tokenization, remove stopwords\n",
    "print(\"Extract tokens from text\",datetime.datetime.now())\n",
    "\n",
    "documentAssembler = DocumentAssembler().setInputCol(\"text\")\\\n",
    "                                       .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetector().setInputCols([\"document\"]) \\\n",
    "                                      .setOutputCol(\"sentence\") \\\n",
    "                                      .setUseAbbreviations(True)\n",
    "\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"])\\\n",
    "                       .setOutputCol(\"token\")\n",
    "\n",
    "\n",
    "# stemmer = Stemmer().setInputCols([\"clean_token\"]) \\\n",
    "#                     .setOutputCol(\"stem\")\n",
    "\n",
    "stopwords_remover =  StopWordsCleaner().setInputCols([\"token\"]) \\\n",
    "                                        .setOutputCol(\"clean_token\") \\\n",
    "                                        .setCaseSensitive(False) \\\n",
    "                                        .setStopWords(stopwords_fr)\n",
    "\n",
    "# lemmatizer = LemmatizerModel.pretrained(\"lemma\", \"fr\")\\\n",
    "#                             .setInputCols([\"clean_token\"])\\\n",
    "#                             .setOutputCol(\"lemma\")\n",
    "\n",
    "\n",
    "finisher = Finisher().setInputCols([\"clean_token\"]) \\\n",
    "                     .setOutputCols([\"clean_token\"])\\\n",
    "                     .setOutputAsArray(True) \\\n",
    "                     .setCleanAnnotations(True)\n",
    "\n",
    "pipeline_prep = sparkMlPipeline(stages=[documentAssembler, tokenizer, stopwords_remover, finisher])\n",
    "\n",
    "df_need = pipeline_prep.fit(df_need).transform(df_need)\n",
    "\n",
    "print(\"Tokens are collected\",datetime.datetime.now())\n",
    "\n",
    "\n",
    "# Lemmatization\n",
    "print(\"Start Lemmatization\",datetime.datetime.now())\n",
    "\n",
    "# To complete the lemmatization we replace some of the words\n",
    "\n",
    "corrections = {\n",
    "    \"trasports\": \"transports\",\n",
    "    \"minibus\":\"bus\",\n",
    "    \"autobus\":\"bus\",\n",
    "    \"car\":\"bus\",\n",
    "    \"cars\":\"bus\",\n",
    "    \n",
    "    \"hopitaux\": \"hôpitaux\",\n",
    "    \"hopital\": \"hôpital\",\n",
    "    \"hospitalier\": \"hôpital\",\n",
    "    \"hospitaliers\": \"hôpital\",\n",
    "    \"médicaux\": \"médecin\",\n",
    "    \"médical\": \"médecin\",\n",
    "    \"médicale\": \"médecin\",\n",
    "    \"medicaux\": \"médecin\",\n",
    "    \"medical\": \"médecin\",\n",
    "    \"medicale\": \"médecin\",\n",
    "    \"medecins\": \"médecin\",\n",
    "    \n",
    "    \"âgées\": \"âgé\",\n",
    "    \"âgés\": \"âgé\",\n",
    "    \"âgée\":\"âgé\",\n",
    "    \"agées\":\"âgé\",\n",
    "    \"agés\": \"âgé\",\n",
    "    \"agée\":\"âgé\",\n",
    "    \n",
    "    \"we\":\"weekend\",\n",
    "    \"week\":\"weekend\",\n",
    "    \"weekends\":\"weekend\",\n",
    "    \n",
    "    \"grises\":\"grise\",\n",
    "    \n",
    "    \"kms\":\"kilomètres\",\n",
    "    \n",
    "#     \"travaillent\":\"travail\",\n",
    "#     \"travailler\":\"travail\",\n",
    "#     \"travail\":\"travail\",\n",
    "    \n",
    "#     \"ferroviaires\":\"ferroviaire\",\n",
    "#     \"ferrées\": \"ferroviaire\",\n",
    "#     \"ferrés\":\"ferroviaire\",\n",
    "    \n",
    "#     \"rurale\":\"rural\",\n",
    "#     \"ruraux\":\"rural\",\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Start lemmatization with custom correction\n",
    "correct_words = F.udf(lambda lst: [corrections.get(s,s) for s in lst], ArrayType(StringType()))\n",
    "df_need  = df_need.withColumn(\"clean_token\", correct_words(df_need.clean_token) )\n",
    "\n",
    "# Lemmatisation : transform token into lemma\n",
    "# We use Claude Coulombe French Lemmatizer\n",
    "lemmatizer = FrenchLefffLemmatizer()\n",
    "lemmatize_udf = F.udf(lambda lst: [lemmatizer.lemmatize(s) for s in lst], ArrayType(StringType()))\n",
    "df_need = df_need.withColumn(\"lemma\", lemmatize_udf(df_need.clean_token))\n",
    "\n",
    "# Remove accents to uniformize tokens/lemma\n",
    "remove_accents = F.udf(lambda lst: [unidecode.unidecode(s) for s in lst],ArrayType(StringType()))\n",
    "df_need  = df_need.withColumn(\"lemma\", remove_accents(df_need.lemma) )\n",
    "\n",
    "# remove_empty = F.udf(lambda lst: [s for s in lst if s.strip()!=''],ArrayType(StringType()))\n",
    "# df_need  = df_need.withColumn(\"lemma\", remove_empty(df_need.lemma) )\n",
    "\n",
    "print(\"Tokens are lemmatized and clean\", datetime.datetime.now())\n",
    "\n",
    "df_need.select(\"q_need_more\", \"lemma\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute TF + LDA 2020-01-30 14:04:33.229626\n",
      "LDA is done 2020-01-30 14:10:16.110254\n",
      "The lower bound on the log likelihood of the entire corpus: -974435.1694153299\n",
      "The upper bound on perplexity: 5.380468727791072\n"
     ]
    }
   ],
   "source": [
    "# We use spark pipeline to apply LDA\n",
    "# 2 main steps\n",
    "# 1) TF (term Frequency) : the frequency of each lemma is computed\n",
    "# 2) LDA is performed\n",
    "\n",
    "print(\"Compute TF + LDA\", datetime.datetime.now())\n",
    "\n",
    "tf = CountVectorizer(vocabSize=300, inputCol='lemma', outputCol='features')\n",
    "\n",
    "idf = IDF(minDocFreq=50, inputCol='features', outputCol='idf')\n",
    "\n",
    "lda = LDA(k=6, maxIter=20)\n",
    "\n",
    "pipeline_analyse = sparkMlPipeline(stages=[tf, idf, lda])\n",
    "\n",
    "model = pipeline_analyse.fit(df_need)\n",
    "df_need = model.transform(df_need)\n",
    "\n",
    "round_vector_udf = F.udf(lambda v: [float(np.round(x,3)) for x in v], ArrayType(FloatType()))\n",
    "df_need = df_need.withColumn(\"topicDistribution\",round_vector_udf(df_need.topicDistribution))\n",
    "\n",
    "argmax_udf = F.udf(lambda lst: float(np.argmax(lst)) if np.sum(lst)>0 else -1, FloatType())\n",
    "df_need = df_need.withColumn(\"max_topic\", argmax_udf(df_need.topicDistribution))\n",
    "\n",
    "print(\"LDA is done\", datetime.datetime.now())\n",
    "\n",
    "# Extract the model\n",
    "lda_model = model.stages[2]\n",
    "\n",
    "ll = lda_model.logLikelihood(df_need)\n",
    "lp = lda_model.logPerplexity(df_need)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "# print(\"The topics described by their top-weighted terms:\")\n",
    "# lda_model.describeTopics(3).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned topics, as distributions over a lexic of 300 words\n",
      "--------------------------------------------------------------------------------------------------------\n",
      " TOPIC 0\n",
      "medecin   sante   hopital   proximite   urgence   poste   specialiste   manque   maison   emploi\n",
      "justice   soin   maternite   education   pole   generaliste   ecole   medecine   ville   moyen\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      " TOPIC 1\n",
      "ville   habite   territoire   securite   region   sociale   habitant   campagne   metropole   pari\n",
      "chance   caf   parisien   ruraux   commune   habiter   manque   departement   province   village\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      " TOPIC 2\n",
      "zone   enfant   commune   ecole   rurales   transport   rurale   place   manque   classe\n",
      "question   age   maison   mairie   eleve   vie   moyen   structure   scolaire   etablissement\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      " TOPIC 3\n",
      "carte   poste   grise   kilometre   prefecture   impot   identite   mairie   commune   sante\n",
      "passeport   police   centre   gendarmerie   rural   medecin   soin   internet   milieu   impots\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      " TOPIC 4\n",
      "internet   demarche   administration   administratif   telephone   reponse   citoyen   numerique   agent   aide\n",
      "accueil   guichet   attente   interlocuteur   physique   contact   age   temps   delai   demande\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------\n",
      " TOPIC 5\n",
      "transport   poste   commun   horaire   bureau   train   bus   ouverture   voiture   heure\n",
      "sncf   travail   kilometre   gare   hopital   ferme   fermeture   jour   ligne   journee\n",
      "--------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Output topics. We display the words that contribute the most to each topic\n",
    "print(\"Learned topics, as distributions over a lexic of {} words\".format(lda_model.vocabSize()))\n",
    "\n",
    "topics = lda_model.describeTopics(20)\n",
    "\n",
    "vocab = model.stages[0].vocabulary\n",
    "\n",
    "topics_words = topics.rdd.map(lambda row: row['termIndices'])\\\n",
    "                         .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "                         .collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "    print(\" TOPIC {}\".format(idx))\n",
    "    print(\"   \".join(topic[:10]))\n",
    "    print(\"   \".join(topic[10:]))\n",
    "    print(\"--------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "En priorité l'hopital et les services liés à la santé.\n",
      "[0.732, 0.052, 0.052, 0.054, 0.055, 0.055]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Renforcer les ressources pour les CCI qui accompagnent tous les jours des chefs d’entreprises qui participent à la création d’emploi sur les territoires\n",
      "[0.884, 0.022, 0.022, 0.023, 0.024, 0.024]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "TOUS. les hopitaux doivent etre une priorite ainsi que les services a la personnes (passerport, CNI, etc..)\n",
      "[0.595, 0.079, 0.078, 0.082, 0.083, 0.083]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Dans la mesure où les études sont gratuites en France, imposer un temps de services (2 à 3 ans) pour tous les métiers en tension, notamment la médecine généraliste et la médecine du travail.\n",
      "[0.885, 0.022, 0.022, 0.023, 0.024, 0.024]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Les déserts médicaux se multiplient (spécialistes en tête, mais généralistes également), or l'activité dans ce domaine est largement permise grâce aux prélèvements obligatoires (argent public). Pourquoi les professionnels de santé ne seraient pas sou\n",
      "[0.577, 0.209, 0.189, 0.008, 0.009, 0.009]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Une police et une justice performantes et expéditives\n",
      "[0.405, 0.052, 0.052, 0.381, 0.055, 0.055]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Hopital\n",
      "[0.595, 0.079, 0.078, 0.082, 0.083, 0.083]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "manque de services public de base ( santé , routes, ..) trop de services public de conseil ou de bonnes consciences ( environnement, logement, anti tabac, anti xx, ..) qui finalement ne servent qu'a une minorité. impossibilité d' avoir un médecin pro\n",
      "[0.643, 0.02, 0.019, 0.02, 0.278, 0.021]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Alors je vais être clair : Je vis à Lyon. Donc normalement je ne devrais pas avoir de problème.  Sauf que : des Services comme l'URSSAF, la CNAM sont injoignables (ou joignable par n° surtaxé !!!), idem pour se soigner :  aucun médecins ne veut prend\n",
      "[0.867, 0.026, 0.026, 0.027, 0.028, 0.027]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "santé,\n",
      "[0.595, 0.079, 0.078, 0.082, 0.083, 0.083]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "les hopitaux, la médecine en général. Suite au départ de généralistes dans les campagnes de nombreuses personnes ne peuvent plus avoir de médecin référant\n",
      "[0.886, 0.022, 0.022, 0.023, 0.023, 0.023]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "L'accès à la médecine est difficile par manque de professionnels en région. Pourquoi ne pas recréer les dispensaires?\n",
      "[0.513, 0.379, 0.026, 0.027, 0.028, 0.027]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Santé en priorité , éducation, transport ensuite tout le reste car ON manque de tout dans certaine région comme le Limousin\n",
      "[0.517, 0.191, 0.026, 0.027, 0.027, 0.212]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "l'accès aux services publics doit être possible facilement en tout point du territoire. Il est important que chaque citoyen puisse avoir accès facilement à ces services (Postes, finances publiques, hôpital, écoles...) Il faut mettre en œuvre un accès\n",
      "[0.633, 0.007, 0.007, 0.007, 0.339, 0.007]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "- Pour les communes péri-urbaines,établir des zones franches sur le principe utilisé pour le plan banlieue, pour y développer des activités commerciales et de service.  \t- lutter contre la désertification des centres villes Ex : créer des « maisons c\n",
      "[0.423, 0.145, 0.41, 0.007, 0.007, 0.007]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Accompagnement plus solide pour l'emploi et la formation.\n",
      "[0.543, 0.295, 0.039, 0.04, 0.041, 0.041]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "activités périscolaires et culturelles,santé, emploi, impôts\n",
      "[0.642, 0.031, 0.228, 0.033, 0.033, 0.033]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Justice, hôpital\n",
      "[0.732, 0.052, 0.052, 0.054, 0.055, 0.055]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "L'éducation. davantage d'écoles et des professeurs titulaires et non pas contractuels!\n",
      "[0.731, 0.052, 0.052, 0.054, 0.055, 0.055]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Santé\n",
      "[0.595, 0.079, 0.078, 0.082, 0.083, 0.083]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# See n examples of text that are more weighted toward topic XX\n",
    "def display_ex(topic, n):\n",
    "    sub = df_need.filter(df_need.max_topic == topic).select(\"q_need_more\", \"topicDistribution\").take(n)\n",
    "    print(\"--------------------------------------------------\")\n",
    "    for x in sub:\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(x[\"q_need_more\"][:250])\n",
    "        print([round(y,3) for y in x[\"topicDistribution\"]])\n",
    "        print(\"--------------------------------------------------\")\n",
    "        \n",
    "display_ex(0,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of answers evoking health services: 9630\n"
     ]
    }
   ],
   "source": [
    "# Classify the answers on whether they talk about healthcare\n",
    "is_topic_present = F.udf(lambda lst: lst[0] > 0.3, BooleanType())\n",
    "\n",
    "df_need = df_need.withColumn(\"evokes_health\", is_topic_present(df_need.topicDistribution))\n",
    "df_need_health =  df_need.filter(df_need.evokes_health == True)\n",
    "print(\"Number of answers evoking health services: {}\".format(df_need_health.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat by zip-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of people evoking healthcare, by zip-code\n",
    "n_health_zc = df_need_health.select(\"zip_code\", 'id').groupby(\"zip_code\").agg(F.count(\"id\").alias(\"n\"))\n",
    "\n",
    "# Geopgraphic location of each zip_code\n",
    "# The dataset indicates the GPS centre of each city in France. Since a zip-code could be used by multiple cities, \n",
    "# we compute the GPS coordinates of one zip-code taking the average GPS coordinates (average latitude, and average longitude)\n",
    "# of cities using this zip-code.\n",
    "\n",
    "# Read data\n",
    "gps_key =  \"code-insee-postaux-geoflar.csv\" \n",
    "gps_location = 's3://{}/{}'.format(bucket, gps_key)\n",
    "gps = spark.read.option(\"header\", \"true\")\\\n",
    "               .option(\"sep\",\";\")\\\n",
    "               .option(\"encoding\",\"utf-8\")\\\n",
    "               .csv(gps_location)\n",
    "\n",
    "gps =  gps.select(\"CODE POSTAL\", \"Geometry X Y\")\n",
    "gps =  gps.withColumnRenamed(\"CODE POSTAL\", \"zip_code\")\n",
    "gps = gps.dropna()\n",
    "\n",
    "# Export Longitude and latitude from the Geoloc\n",
    "get_lat = F.udf(lambda stri: float(stri.split(\",\")[0]), FloatType())\n",
    "get_long = F.udf(lambda stri: float(stri.split(\",\")[1]), FloatType())\n",
    "\n",
    "gps =  gps.withColumn(\"latitude\", get_lat(gps[\"Geometry X Y\"]))\n",
    "gps =  gps.withColumn(\"longitude\", get_long(gps[\"Geometry X Y\"]))\n",
    "\n",
    "# Averaging\n",
    "gps = gps.groupby(\"zip_code\").agg(F.round(F.mean(\"latitude\"),5).alias(\"latitude\"),\n",
    "                                       F.round(F.mean(\"longitude\"),5).alias(\"longitude\"))\n",
    "\n",
    "# We join the dataframe with the numbers of answers in each zip-code, and the GPS coordinates of a zip-code\n",
    "# Some of the lines are lost because the zip-code was incorrect, or because some zip-codes are missing \n",
    "n_health_zc = n_health_zc.join(gps, \"zip_code\", how=\"inner\")\n",
    "\n",
    "# Count by zip-code the total of answers\n",
    "n_tot_zc = df_need.groupby(\"zip_code\").agg(F.count(\"id\").alias(\"n_tot\"))\n",
    "n_health_zc = n_health_zc.join(n_tot_zc, \"zip_code\", how=\"left\")\n",
    "\n",
    "# % of answers in this zip-code evoking \"healthcare\"\n",
    "n_health_zc = n_health_zc.withColumn(\"ratio\", F.round(100*n_health_zc.n/n_health_zc.n_tot,0))\n",
    "\n",
    "# We rescale the ratio, in order to have values between 1 and 30\n",
    "# This weight will define, the size of the marker/dot on the map\n",
    "max_zc = n_health_zc.agg(F.max(\"ratio\").alias('max')).collect()[0]['max']\n",
    "n_health_zc =  n_health_zc.withColumn(\"weight\", F.ceil(n_health_zc.ratio*(30/max_zc)) )\n",
    "n_health_zc.show(5)\n",
    "\n",
    "\n",
    "# Points to plot on map\n",
    "points_zc =  n_health_zc.select(\"n\", \"weight\", \"ratio\",\"latitude\", \"longitude\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAP: count by zip-code\n",
    "mapp = folium.Map(location=[46.724303, 2.496909], zoom_start=6, prefer_canvas=True)\n",
    "for x in points_zc:\n",
    "    folium.CircleMarker(location=[ x[\"latitude\"], x[\"longitude\"] ],\n",
    "                        radius=1,\n",
    "                        weight=x[\"weight\"],\n",
    "                        color=\"red\",\n",
    "                        opacity=0.75).add_to(mapp)\n",
    "\n",
    "mapp.save(\"carte_fr_nombre_parlant_santé.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAP : Choropleth by zip-code (each zone is coloured, depending on how many answers evoked healthcare)\n",
    "\n",
    "# We need to create a pandasdataframe\n",
    "points_zc_df = n_health_zc.toPandas()\n",
    "points_zc_df = points_zc_df.dropna()\n",
    "\n",
    "mapp = folium.Map(location=[46.724303, 2.496909], zoom_start=6, prefer_canvas=True)\n",
    "\n",
    "folium.Choropleth(geo_data=r'contour-des-codes-postaux.geojson',\n",
    "                  data=points_zc_df,\n",
    "                  columns=['zip_code', 'ratio'],\n",
    "                  key_on='feature.properties.code_postal',\n",
    "                  fill_color='RdPu',\n",
    "                  nan_fill_color=\"white\",\n",
    "                  fill_opacity=0.8,\n",
    "                  line_opacity=0.2,\n",
    "                  legend_name='Nombre de réponses évoquant la santé').add_to(mapp)\n",
    "\n",
    "\n",
    "mapp.save(\"carte_fr_choropleth_parlant_santé.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3401.000000\n",
       "mean       42.691855\n",
       "std        24.870896\n",
       "min         5.000000\n",
       "25%        25.000000\n",
       "50%        33.000000\n",
       "75%        50.000000\n",
       "max       100.000000\n",
       "Name: ratio, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_zc_df.ratio.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAP : heatmap\n",
    "heat_data = [[x['latitude'],x['longitude'],x['ratio']] for x in points_zc]\n",
    "\n",
    "mapp = folium.Map(location=[46.724303, 2.496909], zoom_start=6, prefer_canvas=True)\n",
    "\n",
    "foliumplugins.HeatMap(heat_data,radius=10).add_to(mapp)\n",
    "\n",
    "mapp.save(\"carte_fr_heatmap_parlant_santé.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat by department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>n</th>\n",
       "      <th>n_tot</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>58</td>\n",
       "      <td>72</td>\n",
       "      <td>147.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>53</td>\n",
       "      <td>68</td>\n",
       "      <td>146.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>36</td>\n",
       "      <td>61</td>\n",
       "      <td>139.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>47</td>\n",
       "      <td>69</td>\n",
       "      <td>158.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2A</td>\n",
       "      <td>20</td>\n",
       "      <td>46.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>59</td>\n",
       "      <td>135</td>\n",
       "      <td>635.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>13</td>\n",
       "      <td>185</td>\n",
       "      <td>916.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>974</td>\n",
       "      <td>23</td>\n",
       "      <td>117.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>06</td>\n",
       "      <td>93</td>\n",
       "      <td>466.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>44</td>\n",
       "      <td>165</td>\n",
       "      <td>811.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   department    n  n_tot  ratio\n",
       "66         58   72  147.0   49.0\n",
       "63         53   68  146.0   47.0\n",
       "78         36   61  139.0   44.0\n",
       "25         47   69  158.0   44.0\n",
       "48         2A   20   46.0   43.0\n",
       "..        ...  ...    ...    ...\n",
       "15         59  135  635.0   21.0\n",
       "90         13  185  916.0   20.0\n",
       "10        974   23  117.0   20.0\n",
       "69         06   93  466.0   20.0\n",
       "61         44  165  811.0   20.0\n",
       "\n",
       "[101 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of answers about lack of access to health services by department\n",
    "n_health_dep = df_need_health.select(\"department\", 'id').groupby(\"department\").agg(F.count(\"id\").alias(\"n\"))\n",
    "\n",
    "# Count by department on the total of answers\n",
    "n_tot_dep = df_need.groupby(\"department\").agg(F.count(\"id\").alias(\"n_tot\"))\n",
    "# tot_answers = n_tot_dep.agg(F.sum(\"n_tot\").alias(\"n_tot\")).collect()[0][\"n_tot\"]\n",
    "# tot_answers_health = n_health_dep.agg(F.sum(\"n\").alias(\"n\")).collect()[0][\"n\"]\n",
    "\n",
    "# Join the two counts, to get the % of people by department evoking a lack of access to healthcare\n",
    "n_health_dep = n_health_dep.join(n_tot_dep, \"department\", how='left' )\n",
    "\n",
    "# Compute ratio\n",
    "n_health_dep = n_health_dep.withColumn(\"ratio\", F.round(n_health_dep.n / n_health_dep.n_tot ,2)*100)\n",
    "\n",
    "# Transform into pandas.dataframe\n",
    "points_dep_df = n_health_dep.toPandas()\n",
    "points_dep_df = points_dep_df.dropna()\n",
    "points_dep_df = points_dep_df[points_dep_df.n_tot > 10]\n",
    "points_dep_df.sort_values([\"ratio\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geojson_dep = \"contours-departements-fr.geojson\"\n",
    "\n",
    "# Choropleth map\n",
    "mapp = folium.Map(location=[46.724303, 2.496909], zoom_start=6, prefer_canvas=True)\n",
    "\n",
    "folium.Choropleth(geo_data=geojson_dep,\n",
    "                  data=points_dep_df,\n",
    "                  columns=['department', 'ratio'],\n",
    "                  key_on='feature.properties.code_dept',\n",
    "                  fill_color= \"OrRd\",\n",
    "                  nan_fill_color=\"white\",\n",
    "                  fill_opacity=0.8,\n",
    "                  line_opacity=0.2,\n",
    "                  highlight=True,\n",
    "                  legend_name='% de répondant évoquant la santé par les services publics à renforcer').add_to(mapp)\n",
    "\n",
    "\n",
    "mapp.save(\"carte_fr_choropleth_parlant_santé_ratio_dep.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map fro a GeoJSON\n",
    "#  + : we can add a tooltip (popup that appears when hovering) to display some info as the name, the %\n",
    "#      we can have more shaded colors, and use a custom colormap\n",
    "\n",
    "\n",
    "# Append our results in the GeoJSON with the borders of each department\n",
    "geo_dep = gpd.read_file(geojson_dep)\n",
    "geo_dep = geo_dep.merge(points_dep_df[[\"department\",\"n\", \"n_tot\", \"ratio\"]], left_on=\"code_dept\", right_on=\"department\")\n",
    "# geo_dep.to_file(\"health_dep_geo.geojson\", driver='GeoJSON')\n",
    "\n",
    "# Create colormap\n",
    "colormap = folium.LinearColormap(colors=[\"green\",\"yellow\",\"orange\",\"red\"], vmin=20, vmax=50)\n",
    "colormap.caption = \"% de répondants évoquant la santé parmis les services publics à renforcer\"\n",
    "\n",
    "# Create map (looks like a choropleth)\n",
    "# tooltip : what we want in pop-ups\n",
    "mapp = folium.Map(location=[46.724303, 2.496909], zoom_start=6, prefer_canvas=True)\n",
    "\n",
    "folium.GeoJson(data=geo_dep[['geometry','nom_dept',\"ratio\", \"n_tot\"]],\n",
    "               style_function=lambda x: {\"weight\":0.2, 'color':'black','fillColor':colormap(x['properties'][\"ratio\"]), 'fillOpacity':0.8},\n",
    "               highlight_function=lambda x: {'weight':0.5, 'color':'black'},\n",
    "               smooth_factor=2.0,\n",
    "               tooltip=folium.features.GeoJsonTooltip(fields=['nom_dept',\"ratio\",\"n_tot\"],\n",
    "                                               aliases=[\"Nom\", \"% évoquant la santé\",\"Nombre de répondants\"], \n",
    "                                               labels=True, \n",
    "                                               sticky=True)).add_to(mapp)\n",
    "\n",
    "colormap.add_to(mapp)\n",
    "mapp.save(\"carte_fr_choropleth_parlant_santé_ratio_dep2.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[\"#f0d0d0\", \"#f7acac\", \"#f78888\", \"#f76060\",\"#f11212\", \"#bd0e0e\", \"#8c0909\" , \"#660707\", \"#420404\", \"#000000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
